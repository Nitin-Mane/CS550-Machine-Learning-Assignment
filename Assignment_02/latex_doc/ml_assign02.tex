\documentclass{exam}
\usepackage[fontsize=12pt]{scrextend}
\usepackage{xpatch}
\usepackage{babel}
\usepackage{graphicx}
\usepackage{amsmath}  % For mathematical formatting
\usepackage{listings}

\pointsdroppedatright
\marksnotpoints
\makeatletter
\def\mydroppoints{%
  {\unskip\nobreak\hfil\penalty50
    \hskip2em\hbox{}\nobreak\hfil
    (\@points~mark\expandafter\ifx\@points1\else s\fi)
    \parfillskip=0pt \finalhyphendemerits=0 \par}
}
\makeatother

\usepackage{geometry}
\geometry{
  a4paper,% redundant if already in \documentclass
  left=25mm,
  right=25mm,
  top=25mm,
  bottom=25mm,
  heightrounded,% better use it
}

\begin{document}

\begin{center}
\textbf{CS550/DSL501: Machine Learning (2024--25--M)} \\
\textbf{Assignment-II}
\end{center}

\vspace{0.2in}

\begin{center}
  \noindent
  \begin{minipage}[t]{0.49\textwidth}
    \raggedright
    \textsc{\textbf{Full name:}} \text{Nitin Mane}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.49\textwidth}
    \raggedleft
    \textsc{\textbf{ID:}} \textbf{M24MT004}
  \end{minipage}
\end{center}

\vspace{0.2in}

\section{Problem Statement}
\textbf{Imagine a machine learning model designed to diagnose a rare disease that affects only 1\% of the population. In a test set of 10,000 patients, only 100 actually have the disease. The confusion matrix from the model predictions might look like this:}

\begin{itemize}
    \item \textbf{True Positives (TP):} 80 patients correctly diagnosed with the disease.
    \item \textbf{False Negatives (FN):} 20 patients who have the disease but were incorrectly diagnosed as not having it.
    \item \textbf{False Positives (FP):} 100 patients who do not have the disease but were incorrectly diagnosed as having it.
    \item \textbf{True Negatives (TN):} 9,800 patients correctly diagnosed as not having the disease.
\end{itemize}

\textbf{so which performance parameter you should prefer for model performance?}

\subsection{Solution}

Assuming that the parameters listed in the problem statement correspond to the model's output, where performance is quantified using a probabilistic approach based on the model's true values and error rates, we can verify that the performance metrics rely on the predictive outcome, assisting in the identification of better responses that are justified in the model's use case in a real-time setting. In order to do this, we must assess the outcome using the following method of computation.

\subsection*{Mathematical Formulas for Metrics}
Given the confusion matrix, the following metrics can be calculated:

\begin{itemize}
    \item \textbf{Accuracy:} The proportion of correctly classified instances among the total instances.
    \[
    \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
    \]
    
    \item \textbf{Precision:} The proportion of true positives among all predicted positives.
    \[
    \text{Precision} = \frac{TP}{TP + FP}
    \]
    
    \item \textbf{Recall (Sensitivity):} The proportion of true positives among all actual positives.
    \[
    \text{Recall} = \frac{TP}{TP + FN}
    \]
    
    \item \textbf{F1 Score:} The harmonic mean of precision and recall.
    \[
    \text{F1\ Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
    \]
    
    \item \textbf{Mean Squared Error (MSE):} The average squared difference between the actual labels and predictions.
    \[
    \text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (\text{Actual}_i - \text{Predicted}_i)^2
    \]
    
    \item \textbf{Root Mean Squared Error (RMSE):} The square root of the MSE.
    \[
    \text{RMSE} = \sqrt{\text{MSE}}
    \]
\end{itemize}

\subsection*{Code Implementation}
Assume the following confusion matrix values:

\[
\begin{array}{|c|c|}
\hline
\textbf{TP} & 80 \\
\textbf{FN} & 20 \\
\textbf{FP} & 100 \\
\textbf{TN} & 9800 \\
\hline
\end{array}
\]

\begin{lstlisting}[language=Python, caption=Python Code to Calculate Metrics]

# Assume the following values from the confusion matrix
TP = 80
FN = 20
FP = 100
TN = 9800

# Calculate precision = (TP + TN) / (TP + TN + FP + FN)

# Calculate Precision
precision = TP / (TP + FP) if (TP + FP) > 0 else 0

# Calculate Recall
recall = TP / (TP + FN) if (TP + FN) > 0 else 0

# Calculate the F1 score
f1_score = 2 * (precision * recall) / (precision + recall) 
                if (precision + recall) > 0 
                else 0

# Calculate Mean Squared Error (MSE)
actual_labels = [1] * (TP + FN) + [0] * (TN + FP)
predictions = [1] * TP + [0] * FN + [1] * FP + [0] * TN
mse = sum((a - p) ** 2 for a, p in zip(actual_labels, predictions)) / 
len(actual_labels)

# Calculate Root Mean Squared Error (RMSE)
rmse = mse ** 0.5

# Display the results
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1_score:.4f}")
print(f"MSE: {mse:.4f}")
print(f"RMSE: {rmse:.4f}")
\end{lstlisting}

\textbf{Results:}

\begin{itemize}
    \item \textbf{Accuracy:} 0.9880
    \item \textbf{Precision:} 0.4444
    \item \textbf{Recall:} 0.8000
    \item \textbf{F1 Score:} 0.5714
    \item \textbf{MSE:} 0.0120
    \item \textbf{RMSE:} 0.1095
\end{itemize}


\subsection{Answer}
\textit{The optimum performance metric for identifying a rare disease should be \textbf{Recall (Sensitivity)} because missing a positive case (False Negative) can have serious repercussions. The risk of misdiagnosing a patient with the illness is reduced because it guarantees that the majority of true positive cases are appropriately diagnosed. In medical situations where False Negatives could seriously injure patients, it is imperative to prioritize recall.} \hfill \break

\vfill

\textbf{Github Repo :} \newline

\end{document}
